{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining:\n",
    "\n",
    "Text mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\n",
    "\n",
    "Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods.\n",
    "\n",
    "A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.\n",
    "\n",
    "In the previous notebook we have done some Exploratory Data Analysis of various variables including:                  \n",
    "- item_condition_id      \n",
    "- category_name        \n",
    "- brand_name            \n",
    "- price                \n",
    "- shipping               \n",
    "- item_description     \n",
    "present in the dataset to see thier relationship with the target (price) variable. \n",
    "\n",
    "\n",
    "In this notebook I am going to do use text mining to further explore the features and then segregate various products using \n",
    "- LDA \n",
    "- K-means Clustering\n",
    "\n",
    "The following section is based on the tutorial at https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for data manipulations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for dealing with string\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "\n",
    "# for plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style= 'white')\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected = True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "%matplotlib inline\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.transform import  factor_cmap\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "# for textminig \n",
    "from nltk.stem.porter import  *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# for handling warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger('lda').setLevel(logging.WARNING)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the datasets\n",
    "train = pd.read_pickle('./pickle/train.pkl')\n",
    "test = pd.read_pickle('./pickle/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing - Item Description\n",
    "\n",
    "### Pre-processing: tokenization\n",
    "Most of the time, the first steps of an NLP project is to \"tokenize\" your documents, which main purpose is to normalize our texts. The three fundamental stages will usually include:\n",
    "\n",
    "- break the descriptions into sentences and then break the sentences into tokens\n",
    "- remove punctuation and stop words\n",
    "- lowercase the tokens\n",
    "- herein, I will also only consider words that have length equal to or greater than 3 characters (3- grams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    sent_tokenize(): segments text into sentences\n",
    "    word_tokenize(): breaks sentences into words\n",
    "    \"\"\"\n",
    "    try: \n",
    "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        \n",
    "        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop_words, tokens))\n",
    "        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n",
    "        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n",
    "        \n",
    "        return filtered_tokens\n",
    "            \n",
    "    except TypeError as e: \n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply the tokenizer into the item descriptipn column\n",
    "# train['tokens'] = train['item_description'].map(tokenize)\n",
    "# test['tokens'] = test['item_description'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "      <th>general_cat</th>\n",
       "      <th>subcat_1</th>\n",
       "      <th>subcat_2</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "      <td>Men</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-shirts</td>\n",
       "      <td>1</td>\n",
       "      <td>[description, yet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>Razer</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Computers &amp; Tablets</td>\n",
       "      <td>Components &amp; Parts</td>\n",
       "      <td>14</td>\n",
       "      <td>[keyboard, great, condition, works, like, came...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Target</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "      <td>Women</td>\n",
       "      <td>Tops &amp; Blouses</td>\n",
       "      <td>Blouse</td>\n",
       "      <td>8</td>\n",
       "      <td>[adorable, top, hint, lace, key, hole, back, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>1</td>\n",
       "      <td>Home/Home Décor/Home Décor Accents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Home Décor</td>\n",
       "      <td>Home Décor Accents</td>\n",
       "      <td>14</td>\n",
       "      <td>[new, tags, leather, horses, retail, stand, fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "      <td>Women</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>Necklaces</td>\n",
       "      <td>3</td>\n",
       "      <td>[complete, certificate, authenticity]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                                 name  item_condition_id  \\\n",
       "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2         2                       AVA-VIV Blouse                  1   \n",
       "3         3                Leather Horse Statues                  1   \n",
       "4         4                 24K GOLD plated rose                  1   \n",
       "\n",
       "                                       category_name brand_name  price  \\\n",
       "0                                  Men/Tops/T-shirts        NaN   10.0   \n",
       "1  Electronics/Computers & Tablets/Components & P...      Razer   52.0   \n",
       "2                        Women/Tops & Blouses/Blouse     Target   10.0   \n",
       "3                 Home/Home Décor/Home Décor Accents        NaN   35.0   \n",
       "4                            Women/Jewelry/Necklaces        NaN   44.0   \n",
       "\n",
       "   shipping                                   item_description  general_cat  \\\n",
       "0         1                                 No description yet          Men   \n",
       "1         0  This keyboard is in great condition and works ...  Electronics   \n",
       "2         1  Adorable top with a hint of lace and a key hol...        Women   \n",
       "3         1  New with tags. Leather horses. Retail for [rm]...         Home   \n",
       "4         0          Complete with certificate of authenticity        Women   \n",
       "\n",
       "              subcat_1            subcat_2  desc_len  \\\n",
       "0                 Tops            T-shirts         1   \n",
       "1  Computers & Tablets  Components & Parts        14   \n",
       "2       Tops & Blouses              Blouse         8   \n",
       "3           Home Décor  Home Décor Accents        14   \n",
       "4              Jewelry           Necklaces         3   \n",
       "\n",
       "                                              tokens  \n",
       "0                                 [description, yet]  \n",
       "1  [keyboard, great, condition, works, like, came...  \n",
       "2  [adorable, top, hint, lace, key, hole, back, p...  \n",
       "3  [new, tags, leather, horses, retail, stand, fo...  \n",
       "4              [complete, certificate, authenticity]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the train data into pickle that can be loaded later.\n",
    "train.to_pickle('./pickle/train_token.pkl')\n",
    "test.to_pickle('./pickle/test_token.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description: No description yet\n",
      "tokens: ['description', 'yet']\n",
      "\n",
      "description: This keyboard is in great condition and works like it came out of the box. All of the ports are tested and work perfectly. The lights are customizable via the Razer Synapse app on your PC.\n",
      "tokens: ['keyboard', 'great', 'condition', 'works', 'like', 'came', 'box', 'ports', 'tested', 'work', 'perfectly', 'lights', 'customizable', 'via', 'razer', 'synapse', 'app']\n",
      "\n",
      "description: Adorable top with a hint of lace and a key hole in the back! The pale pink is a 1X, and I also have a 3X available in white!\n",
      "tokens: ['adorable', 'top', 'hint', 'lace', 'key', 'hole', 'back', 'pale', 'pink', 'also', 'available', 'white']\n",
      "\n",
      "description: New with tags. Leather horses. Retail for [rm] each. Stand about a foot high. They are being sold as a pair. Any questions please ask. Free shipping. Just got out of storage\n",
      "tokens: ['new', 'tags', 'leather', 'horses', 'retail', 'stand', 'foot', 'high', 'sold', 'pair', 'questions', 'please', 'ask', 'free', 'shipping', 'got', 'storage']\n",
      "\n",
      "description: Complete with certificate of authenticity\n",
      "tokens: ['complete', 'certificate', 'authenticity']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets look at some descriptions along with its tokens\n",
    "for description, tokens in zip(train['item_description'].head(),\n",
    "                              train['tokens'].head()):\n",
    "    print('description:', description)\n",
    "    print('tokens:', tokens)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Women', 'Beauty', 'Kids', 'Electronics', 'Men'], dtype='<U11')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see the most common occuring words in each categories\n",
    "general_cats = train['general_cat'].value_counts().head(5).index.values.astype('str')\n",
    "general_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a dict with key = category and value = all the descriptions for that category\n",
    "cat_desc = dict()\n",
    "for cat in general_cats: \n",
    "    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n",
    "    cat_desc[cat] = tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most common words for the top 4 categories\n",
    "women100 = Counter(cat_desc['Women']).most_common(100)\n",
    "beauty100 = Counter(cat_desc['Beauty']).most_common(100)\n",
    "kids100 = Counter(cat_desc['Kids']).most_common(100)\n",
    "electronics100 = Counter(cat_desc['Electronics']).most_common(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(tup):\n",
    "    wordcloud = WordCloud(background_color='white',\n",
    "                          max_words=50, max_font_size=40,\n",
    "                          random_state=42\n",
    "                         ).generate(str(tup))\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2, 2, figsize=(30, 15))\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.imshow(generate_wordcloud(women100), interpolation=\"bilinear\")\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Women Top 100\", fontsize=30)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.imshow(generate_wordcloud(beauty100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Beauty Top 100\", fontsize=30)\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.imshow(generate_wordcloud(kids100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Kids Top 100\", fontsize=30)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.imshow(generate_wordcloud(electronics100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Electronic Top 100\", fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: tf-idf\n",
    "\n",
    "tf-idf is the acronym for Term Frequency–inverse Document Frequency. It quantifies the importance of a particular word relative to the vocabulary of a collection of documents or corpus. The metric depends on two factors:\n",
    "\n",
    "- <b>Term Frequency</b>: the occurences of a word in a given document (i.e. bag of words)\n",
    "- <b>Inverse Document Frequency</b>: the reciprocal number of times a word occurs in a corpus of documents\n",
    "\n",
    "Think about of it this way: If the word is used extensively in all documents, its existence within a specific document will not be able to provide us much specific information about the document itself. So the second term could be seen as a penalty term that penalizes common words such as \"a\", \"the\", \"and\", etc. tf-idf can therefore, be seen as a weighting scheme for words relevancy in a specific document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df= 10,\n",
    "                            max_features=180000,\n",
    "                            tokenizer= tokenize,\n",
    "                            ngram_range=(1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_desc = np.append(train['item_description'].values, test['item_description'].values)\n",
    "vz = vectorizer.fit_transform(list(all_desc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vz is a tfidf matrix where:\n",
    "\n",
    "- the number of rows is the total number of descriptions\n",
    "- the number of columns is the total number of unique tokens across the descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2175890, 180000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a dictionary mapping the tokens to their tfidf values\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "tfidf = pd.DataFrame(columns=['tfidf']).from_dict(\n",
    "                    dict(tfidf), orient='index')\n",
    "tfidf.columns = ['tfidf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the 10 tokens with the lowest tfidf score, which is unsurprisingly, very generic words that we could not use to distinguish one description from another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>2.175653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>2.330674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>2.755660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition</th>\n",
       "      <td>2.799306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand new</th>\n",
       "      <td>2.874418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>2.903426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shipping</th>\n",
       "      <td>3.070592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worn</th>\n",
       "      <td>3.107882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>3.165310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never</th>\n",
       "      <td>3.276901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tfidf\n",
       "new        2.175653\n",
       "size       2.330674\n",
       "brand      2.755660\n",
       "condition  2.799306\n",
       "brand new  2.874418\n",
       "free       2.903426\n",
       "shipping   3.070592\n",
       "worn       3.107882\n",
       "used       3.165310\n",
       "never      3.276901"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.sort_values(by=['tfidf'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the 10 tokens with the highest tfidf score, which includes words that are a lot specific that by looking at them, we could guess the categories that they belong to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>silver pcs pcs</th>\n",
       "      <td>13.195054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cases pairs cases</th>\n",
       "      <td>13.195054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beats beats</th>\n",
       "      <td>13.195054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waist size order</th>\n",
       "      <td>13.195054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>china slim</th>\n",
       "      <td>13.108042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>order size waist</th>\n",
       "      <td>13.108042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hair diameter</th>\n",
       "      <td>13.108042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shimmering finish</th>\n",
       "      <td>13.108042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size order size</th>\n",
       "      <td>13.027999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slim tea</th>\n",
       "      <td>13.027999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       tfidf\n",
       "silver pcs pcs     13.195054\n",
       "cases pairs cases  13.195054\n",
       "beats beats        13.195054\n",
       "waist size order   13.195054\n",
       "china slim         13.108042\n",
       "order size waist   13.108042\n",
       "hair diameter      13.108042\n",
       "shimmering finish  13.108042\n",
       "size order size    13.027999\n",
       "slim tea           13.027999"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.sort_values(by=['tfidf'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the high dimension of our tfidf matrix, we need to reduce their dimension using the Singular Value Decomposition (SVD) technique. And to visualize our vocabulary, we could next use t-SNE to reduce the dimension from 50 to 2. t-SNE is more suitable for dimensionality reduction to 2 or 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>t-Distributed Stochastic Neighbor Embedding (t-SNE)</b>\n",
    "\n",
    "t-SNE is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. The goal is to take a set of points in a high-dimensional space and find a representation of those points in a lower-dimensional space, typically the 2D plane. It is based on probability distributions with random walk on neighborhood graphs to find the structure within the data. But since t-SNE complexity is significantly high, usually we'd use other high-dimension reduction techniques before applying t-SNE.\n",
    "\n",
    "First, let's take a sample from the both training and testing item's description since t-SNE can take a very long time to execute. We can then reduce the dimension of each vector from to n_components (50) using SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = train.copy()\n",
    "tst = test.copy()\n",
    "trn['is_train'] = 1\n",
    "tst['is_train'] = 0\n",
    "\n",
    "sample_sz = 15000\n",
    "\n",
    "combined_df = pd.concat([trn, tst])\n",
    "combined_sample = combined_df.sample(n=sample_sz)\n",
    "vz_sample = vectorizer.fit_transform(list(combined_sample['item_description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_comp=30\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "svd_tfidf = svd.fit_transform(vz_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reduce the dimension from 50 to 2 using t-SNE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 15000 samples in 0.022s...\n",
      "[t-SNE] Computed neighbors for 15000 samples in 9.927s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 15000\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 89.281303\n",
      "[t-SNE] Error after 500 iterations: 1.908422\n"
     ]
    }
   ],
   "source": [
    "tsne_tfidf = tsne_model.fit_transform(svd_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now possible to visualize our data points. Note that the deviation as well as the size of the clusters imply little information in t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"17df1c87-e349-4893-a945-c944ec7faac2\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"17df1c87-e349-4893-a945-c944ec7faac2\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"17df1c87-e349-4893-a945-c944ec7faac2\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '17df1c87-e349-4893-a945-c944ec7faac2' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.13.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"17df1c87-e349-4893-a945-c944ec7faac2\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"17df1c87-e349-4893-a945-c944ec7faac2\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"17df1c87-e349-4893-a945-c944ec7faac2\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '17df1c87-e349-4893-a945-c944ec7faac2' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.13.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"17df1c87-e349-4893-a945-c944ec7faac2\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()\n",
    "plot_tfidf = bp.figure(plot_width=700, plot_height=600,\n",
    "                       title=\"tf-idf clustering of the item description\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sample.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])\n",
    "tfidf_df['description'] = combined_sample['item_description']\n",
    "tfidf_df['tokens'] = combined_sample['tokens']\n",
    "tfidf_df['category'] = combined_sample['general_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tfidf.scatter(x='x', y='y', source=tfidf_df, alpha=0.7)\n",
    "hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"description\": \"@description\", \"tokens\": \"@tokens\", \"category\":\"@category\"}\n",
    "show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "K-means clustering obejctive is to minimize the average squared Euclidean distance of the document / description from their cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "num_clusters = 30 # need to be selected wisely\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=num_clusters,\n",
    "                               init='k-means++',\n",
    "                               n_init=1,\n",
    "                               init_size=1000, batch_size=1000, verbose=0, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = kmeans_model.fit(vz)\n",
    "kmeans_clusters = kmeans.predict(vz)\n",
    "kmeans_distances = kmeans.transform(vz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "# terms = vectorizer.get_feature_names()\n",
    "\n",
    "# for i in range(num_clusters):\n",
    "#     print(\"Cluster %d:\" % i)\n",
    "#     aux = ''\n",
    "#     for j in sorted_centroids[i, :10]:\n",
    "#         aux += terms[j] + ' | '\n",
    "#     print(aux)\n",
    "#     print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 15000 samples in 0.048s...\n",
      "[t-SNE] Computed neighbors for 15000 samples in 7.528s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 15000\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 85.445244\n",
      "[t-SNE] Error after 500 iterations: 1.821069\n"
     ]
    }
   ],
   "source": [
    "# repeat the same steps for the sample\n",
    "kmeans = kmeans_model.fit(vz_sample)\n",
    "kmeans_clusters = kmeans.predict(vz_sample)\n",
    "kmeans_distances = kmeans.transform(vz_sample)\n",
    "# reduce dimension to 2 using tsne\n",
    "tsne_kmeans = tsne_model.fit_transform(kmeans_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n",
    "\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n",
    "\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n",
    "\"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_sample.reset_index(drop=True, inplace=True)\n",
    "kmeans_df = pd.DataFrame(tsne_kmeans, columns=['x', 'y'])\n",
    "kmeans_df['cluster'] = kmeans_clusters\n",
    "kmeans_df['description'] = combined_sample['item_description']\n",
    "kmeans_df['category'] = combined_sample['general_cat']\n",
    "#kmeans_df['cluster']=kmeans_df.cluster.astype(str).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans = bp.figure(plot_width=700, plot_height=600,\n",
    "                        title=\"KMeans clustering of the description\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(data=dict(x=kmeans_df['x'], y=kmeans_df['y'],\n",
    "                                    color=colormap[kmeans_clusters],\n",
    "                                    description=kmeans_df['description'],\n",
    "                                    category=kmeans_df['category'],\n",
    "                                    cluster=kmeans_df['cluster']))\n",
    "\n",
    "plot_kmeans.scatter(x='x', y='y', color='color', source=source)\n",
    "hover = plot_kmeans.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"description\": \"@description\", \"category\": \"@category\", \"cluster\":\"@cluster\" }\n",
    "show(plot_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is an algorithms used to discover the topics that are present in a corpus.\n",
    "\n",
    "LDA starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents.\n",
    "\n",
    "Reference: https://medium.com/intuitionmachine/the-two-paths-from-natural-language-processing-to-artificial-intelligence-d5384ddbfc18\n",
    "Its input is a bag of words, i.e. each document represented as a row, with each columns containing the count of words in the corpus. We are going to use a powerful tool called pyLDAvis that gives us an interactive visualization for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvectorizer = CountVectorizer(min_df=4,\n",
    "                              max_features=180000,\n",
    "                              tokenizer=tokenize,\n",
    "                              ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvz = cvectorizer.fit_transform(combined_sample['item_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=20,\n",
    "                                      learning_method='online',\n",
    "                                      max_iter=20,\n",
    "                                      random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_topics = lda_model.fit_transform(cvz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: still | new | fast | package | card | also | shipping | find | items | pack\n",
      "Topic 1: back | one | included | front | waist | baby | pocket | pockets | pic | top\n",
      "Topic 2: nwt | disney | bikini | fading | slight | rings | reserved | runs | lightweight | mailers\n",
      "Topic 3: new | brand | brand new | price | tags | free | shipping | firm | free shipping | size\n",
      "Topic 4: jeans | big | stretch | mint | fit | games | forever | hollister | awesome | skinny\n",
      "Topic 5: size | cute | super | small | leggings | worn | dress | soft | fits | lularoe\n",
      "Topic 6: never | used | worn | never worn | never used | new never | size | medium | box | new\n",
      "Topic 7: iphone | phone | charger | apple | glass | iphone plus | clean | screen | battery | iphone iphone\n",
      "Topic 8: condition | size | great | good | great condition | good condition | large | worn | small | times\n",
      "Topic 9: brown | body | colors | couple | light | shade | authentic | cream | palette | beige\n",
      "Topic 10: description | yet | bundle | description yet | shipping | save | please | items | make | bundle save\n",
      "Topic 11: high | long | quality | sleeve | used | gently | use | pink | hair | one\n",
      "Topic 12: skin | color | brush | makeup | lip | wore | matte | eye | use | natural\n",
      "Topic 13: red | gift | great | beautiful | color | full | looks | unicorn | tested | colors\n",
      "Topic 14: blue | color | top | navy | white | green | light | band | black | inch\n",
      "Topic 15: gold | case | check | listings | check listings | rose | tee | edition | cases | wallet\n",
      "Topic 16: please | free | shipping | ship | day | ask | questions | see | buy | feel\n",
      "Topic 17: free | home | smoke | free home | smoke free | pet | pet free | game | comes | ring\n",
      "Topic 18: pink | new | like | black | like new | secret | white | victoria | victoria secret | size\n",
      "Topic 19: shirt | grey | many | short | look | amazing | vintage | sweater | book | love\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.components_  # get the topic words\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 15000 samples in 0.016s...\n",
      "[t-SNE] Computed neighbors for 15000 samples in 5.526s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 15000\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 15000\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 93.199722\n",
      "[t-SNE] Error after 500 iterations: 2.389401\n"
     ]
    }
   ],
   "source": [
    "# reduce dimension to 2 using tsne\n",
    "tsne_lda = tsne_model.fit_transform(X_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized = np.matrix(X_topics)\n",
    "doc_topic = unnormalized/unnormalized.sum(axis=1)\n",
    "\n",
    "lda_keys = []\n",
    "for i, tweet in enumerate(combined_sample['item_description']):\n",
    "    lda_keys += [doc_topic[i].argmax()]\n",
    "\n",
    "lda_df = pd.DataFrame(tsne_lda, columns=['x','y'])\n",
    "lda_df['description'] = combined_sample['item_description']\n",
    "lda_df['category'] = combined_sample['general_cat']\n",
    "lda_df['topic'] = lda_keys\n",
    "lda_df['topic'] = lda_df['topic'].map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lda = bp.figure(plot_width=700,\n",
    "                     plot_height=600,\n",
    "                     title=\"LDA topic visualization\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(data=dict(x=lda_df['x'], y=lda_df['y'],\n",
    "                                    color=colormap[lda_keys],\n",
    "                                    description=lda_df['description'],\n",
    "                                    topic=lda_df['topic'],\n",
    "                                    category=lda_df['category']))\n",
    "\n",
    "plot_lda.scatter(source=source, x='x', y='y', color='color')\n",
    "hover = plot_kmeans.select(dict(type=HoverTool))\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"description\":\"@description\",\n",
    "                \"topic\":\"@topic\", \"category\":\"@category\"}\n",
    "show(plot_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareLDAData():\n",
    "    data = {\n",
    "        'vocab': vocab,\n",
    "        'doc_topic_dists': doc_topic,\n",
    "        'doc_lengths': list(lda_df['len_docs']),\n",
    "        'term_frequency':cvectorizer.vocabulary_,\n",
    "        'topic_term_dists': lda_model.components_\n",
    "    } \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "\n",
    "lda_df['len_docs'] = combined_sample['tokens'].map(len)\n",
    "ldadata = prepareLDAData()\n",
    "pyLDAvis.enable_notebook()\n",
    "prepared_data = pyLDAvis.prepare(**ldadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "from IPython.core.display import display, HTML, Javascript\n",
    "\n",
    "h = IPython.display.display(HTML(html_string))\n",
    "IPython.display.display_HTML(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
